{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>P_Number</th>\n",
       "      <th>birth</th>\n",
       "      <th>data_exam</th>\n",
       "      <th>CMRC_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>data_year</th>\n",
       "      <th>year_come</th>\n",
       "      <th>height_y</th>\n",
       "      <th>...</th>\n",
       "      <th>腎結節  ( yes=1, no=0)</th>\n",
       "      <th>左腎 (無:0, 單一:1,多發:2).3</th>\n",
       "      <th>公分.23</th>\n",
       "      <th>右腎 (無:0, 單一:1,多發:2).3</th>\n",
       "      <th>公分.24</th>\n",
       "      <th>脾臟 (0:正常;1:脾臟腫大；2脾臟切除)</th>\n",
       "      <th>spleen long axis (cm)</th>\n",
       "      <th>spleen short axis (cm)</th>\n",
       "      <th>脾面積(大於20CM=脾腫大)</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M11A011</td>\n",
       "      <td>M11A011</td>\n",
       "      <td>391127</td>\n",
       "      <td>20221122</td>\n",
       "      <td>R8M080Y5005M5</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>145.2</td>\n",
       "      <td>...</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20221119122</td>\n",
       "      <td>P111782</td>\n",
       "      <td>410428</td>\n",
       "      <td>20221119</td>\n",
       "      <td>E42018P21120K8045</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>153.5</td>\n",
       "      <td>...</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20221119121</td>\n",
       "      <td>P111781</td>\n",
       "      <td>401120</td>\n",
       "      <td>20221119</td>\n",
       "      <td>H62016G10710R5042</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>147.5</td>\n",
       "      <td>...</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20221119120</td>\n",
       "      <td>P111780</td>\n",
       "      <td>430325</td>\n",
       "      <td>20221119</td>\n",
       "      <td>U32016A80710B9013</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20221119119</td>\n",
       "      <td>P111779</td>\n",
       "      <td>390220</td>\n",
       "      <td>20221119</td>\n",
       "      <td>A72018S91120Z2060</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>157</td>\n",
       "      <td>...</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 954 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sid P_Number   birth  data_exam            CMRC_id  sex  age   \n",
       "0      M11A011  M11A011  391127   20221122      R8M080Y5005M5    0   71  \\\n",
       "1  20221119122  P111782  410428   20221119  E42018P21120K8045    0   70   \n",
       "2  20221119121  P111781  401120   20221119  H62016G10710R5042    0   70   \n",
       "3  20221119120  P111780  430325   20221119  U32016A80710B9013    0   68   \n",
       "4  20221119119  P111779  390220   20221119  A72018S91120Z2060    1   72   \n",
       "\n",
       "   data_year  year_come height_y  ... 腎結節  ( yes=1, no=0)   \n",
       "0       2022          2    145.2  ...                  \\N  \\\n",
       "1       2022          3    153.5  ...                  \\N   \n",
       "2       2022          5    147.5  ...                  \\N   \n",
       "3       2022          4      148  ...                  \\N   \n",
       "4       2022          3      157  ...                  \\N   \n",
       "\n",
       "  左腎 (無:0, 單一:1,多發:2).3 公分.23 右腎 (無:0, 單一:1,多發:2).3 公分.24   \n",
       "0                    \\N    \\N                    \\N    \\N  \\\n",
       "1                    \\N    \\N                    \\N    \\N   \n",
       "2                    \\N    \\N                    \\N    \\N   \n",
       "3                    \\N    \\N                    \\N    \\N   \n",
       "4                    \\N    \\N                    \\N    \\N   \n",
       "\n",
       "  脾臟 (0:正常;1:脾臟腫大；2脾臟切除) spleen long axis (cm) spleen short axis (cm)   \n",
       "0                     \\N                    \\N                     \\N  \\\n",
       "1                     \\N                    \\N                     \\N   \n",
       "2                     \\N                    \\N                     \\N   \n",
       "3                     \\N                    \\N                     \\N   \n",
       "4                     \\N                    \\N                     \\N   \n",
       "\n",
       "  脾面積(大於20CM=脾腫大) other  \n",
       "0              \\N    \\N  \n",
       "1              \\N    \\N  \n",
       "2              \\N    \\N  \n",
       "3              \\N    \\N  \n",
       "4              \\N    \\N  \n",
       "\n",
       "[5 rows x 954 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read dataset as df\n",
    "df = pd.read_excel(\"../data/NTCMRC_all.xlsx\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utilities import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sizhechen/Desktop/git/fatty-liver-project/src/features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sizhechen/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "<ipython-input-7-ab4dc1e13a1c>:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df9['t3_MAFLD'] = df8['t3_MAFLD']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1_sex</th>\n",
       "      <th>t1_age</th>\n",
       "      <th>t1_waist_y</th>\n",
       "      <th>t1_Glucose_AC_y</th>\n",
       "      <th>t1_Triglyceride_y</th>\n",
       "      <th>t1_HDL_C_y</th>\n",
       "      <th>t1_AST_GOT</th>\n",
       "      <th>t1_ALT_GPT</th>\n",
       "      <th>t1_gamgt</th>\n",
       "      <th>t1_Insulin</th>\n",
       "      <th>...</th>\n",
       "      <th>t2_sarcf</th>\n",
       "      <th>t2_ms2</th>\n",
       "      <th>t2_MNA</th>\n",
       "      <th>t2_AUDIT</th>\n",
       "      <th>t2_HBV_</th>\n",
       "      <th>t2_HCV_</th>\n",
       "      <th>t2_CKD</th>\n",
       "      <th>t2_HBsAg_x_num</th>\n",
       "      <th>t2_Anti_HCV_x_num</th>\n",
       "      <th>t3_MAFLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>82.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>56.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>83.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>55.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>85.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>60.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>84.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>61.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>60.4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.75</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   t1_sex  t1_age  t1_waist_y  t1_Glucose_AC_y  t1_Triglyceride_y  t1_HDL_C_y   \n",
       "0       0      59        82.0            101.0              264.0        56.4  \\\n",
       "1       0      60        83.0            104.0              255.0        55.8   \n",
       "2       0      62        85.0            103.0              192.0        60.7   \n",
       "3       0      63        84.0            111.0              164.0        83.0   \n",
       "4       0      58        61.0             99.0               75.0        60.4   \n",
       "\n",
       "   t1_AST_GOT  t1_ALT_GPT  t1_gamgt  t1_Insulin  ...  t2_sarcf  t2_ms2   \n",
       "0        21.0        19.0      15.0        9.18  ...       NaN       1  \\\n",
       "1        15.0        18.0      14.0        5.86  ...       NaN       1   \n",
       "2        18.0        22.0      17.0        4.56  ...       NaN       1   \n",
       "3        27.0        30.0      16.0        6.30  ...       NaN       1   \n",
       "4        23.0        17.0      10.0        3.75  ...       NaN       0   \n",
       "\n",
       "   t2_MNA  t2_AUDIT  t2_HBV_  t2_HCV_  t2_CKD  t2_HBsAg_x_num   \n",
       "0     NaN       1.0      NaN      NaN       1             NaN  \\\n",
       "1     NaN       0.0      0.0      0.0       2            0.48   \n",
       "2     NaN       0.0      NaN      NaN       2             NaN   \n",
       "3     NaN       0.0      0.0      0.0       2            0.33   \n",
       "4     NaN       1.0      NaN      NaN       2             NaN   \n",
       "\n",
       "   t2_Anti_HCV_x_num  t3_MAFLD  \n",
       "0                NaN         0  \n",
       "1              0.030         0  \n",
       "2                NaN         1  \n",
       "3              0.036         0  \n",
       "4                NaN         0  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a copy of df as df1\n",
    "df1 = df.copy()\n",
    "\n",
    "# Replace '\\\\N' with NaN\n",
    "df1 = df1.replace('\\\\N', np.nan)\n",
    "\n",
    "# Specify the columns to be converted to FLOAT\n",
    "columns_to_convert1 = ['BMI', 'Triglyceride_y', 'gamgt', 'waist_y', 'mst', 'egfrn', 'Estimated_GFR_x', 'Alb_Cre_ratio', 'HOMA_IR', 'HS_CRP', \\\n",
    "                       'LDL_C_direct', 'LDL_C_HDL_C', 'Adiponectin', 'Leptin', 'Uric_Acid','Insulin', 'ALT_GPT']\n",
    "\n",
    "# Specify the columns to be converted to INT\n",
    "columns_to_convert2 = ['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)', 'smoke', 'smoke_q', \\\n",
    "                       'sex', 'w', 'coffee', 'betel']\n",
    "\n",
    "# Convert the specified columns to float and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert1:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce')\n",
    "\n",
    "# Convert the specified columns to int and fill missing/unconvertible values with NaN\n",
    "for column in columns_to_convert2:\n",
    "    df1[column] = pd.to_numeric(df1[column], errors='coerce').astype(pd.Int64Dtype())\n",
    "\n",
    "# Calculate FLI using the formula and defined as df2\n",
    "df2 = df1.copy()\n",
    "df2['FLI'] = (np.exp(0.953 * np.log(df2['Triglyceride_y']) + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) \\\n",
    "     + 0.053 * df2['waist_y'] - 15.745)) / (1 + np.exp(0.953 * np.log(df2['Triglyceride_y']) \\\n",
    "    + 0.139 * df2['BMI'] + 0.718 * np.log(df2['gamgt']) + 0.053 * df2['waist_y'] - 15.745)) * 100\n",
    "\n",
    "# Derive FL_echo based on ultrasound results column\n",
    "df2['FL_echo'] = df2['脂肪肝 fatty Liver (0:正常  1:mild 2:moderate 3:severe)']\n",
    "df2['FL_echo'] = df2['FL_echo'].replace('<NA>', np.nan)\n",
    "df2['fl_status'] = df2.apply(utils.derive_fl_status, axis=1)\n",
    "\n",
    "#Derive homa_ir_check, hs_crp_check, and mst_total to determine MAFLD risk factors\n",
    "df2['homa_ir_check'] = df2['HOMA_IR'].apply(lambda x: 1 if x >= 2.5 else 0)\n",
    "df2['hs_crp_check'] = df2['HS_CRP'].apply(lambda x: 1 if x > 2 else 0)\n",
    "df2['mst_total'] = df2[['w', 'hyper', 'HDL', 'fg', 'trig', 'homa_ir_check', 'hs_crp_check']].sum(axis=1)\n",
    "\n",
    "\n",
    "df3 = utils.derive_CKD(utils.derive_MAFLD(df2))\n",
    "\n",
    "# Derive FL_group_list, 对CMRC_id分组并计算每个病人的FL_Check的唯一值\n",
    "grouped = df2.groupby('CMRC_id')['fl_status'].unique()\n",
    "df2['fl_group_list'] = df2.groupby('CMRC_id')['fl_status'].transform(lambda x: [x.unique().tolist()] * len(x))\n",
    "\n",
    "#assign patient valid value\n",
    "df4 = utils.assign_patient_fl_validity(df3)\n",
    "\n",
    "# Deal with values with both string and num values: HBsAg_x, Anti_HCV_x, (values eg. 陰性    0.351)\n",
    "# extract numeric values for these cols and rename that as  *_num\n",
    "columns_to_extract = ['HBsAg_x', 'Anti_HCV_x']\n",
    "df5 = df4.copy()\n",
    "for column in columns_to_extract:\n",
    "    new_column_name = column + '_num'\n",
    "    df5[new_column_name] = df5[column].apply(utils.extract_numeric_value)\n",
    "\n",
    "# Sliding window implementation, ie. use previous 2 years record to predict the 3 year MAFLD status\n",
    "df6 = utils.sliding_window_data(df5, input_window_size=2, target_window_size=1)\n",
    "\n",
    "# Filter available data that can be applied to models\n",
    "df7 = df6[(df6['t3_MAFLD'] != -1)]\n",
    "\n",
    "# Drop ID relevant cols in the dataset\n",
    "columns_to_drop = ['CMRC_id', 't1_CMRC_id', 't2_CMRC_id', 't1_sid', 't2_sid', 't1_P_Number','t2_P_Number']\n",
    "df8 = df7.drop(columns=columns_to_drop)\n",
    "\n",
    "#Select key columns for conventional machine learning models\n",
    "columns = [\"sex\", \"age\", \"waist_y\", \"Glucose_AC_y\", \"Triglyceride_y\", \"HDL_C_y\", \"AST_GOT\", \"ALT_GPT\", \\\n",
    "          \"gamgt\", \"Insulin\", \"T_Cholesterol\", \"LDL_C_direct\", \"VLDL_C\", \"Non_HDL_C\", \"T_CHOL_HDL_C\", \\\n",
    "          \"LDL_C_HDL_C\", \"HS_CRP\", \"Hb_A1c\", \"Uric_Acid\", \"HBsAg_x\", \"Anti_HCV_x\", \"HOMA_IR\", \"Adiponectin\", \\\n",
    "           \"Leptin\", \"TotalVitaminD\", \"smoke\", \"smoke_q\", \"coffee\", \"betel\", \"BMI\", \"DM_determine\", \"w\", \"hyper\", \\\n",
    "           \"fg\", \"HDL\", \"trig\", \"sarcf\", \"ms2\", \"MNA\", \"AUDIT\", \"HBV_\", \"HCV_\", \"MAFLD\", \"CKD\", \\\n",
    "           'HBsAg_x_num', 'Anti_HCV_x_num']\n",
    "prefixes = [\"t1_\", \"t2_\"]\n",
    "renamed_columns = utils.add_prefix(columns, prefixes)\n",
    "\n",
    "df9 = df8[renamed_columns]\n",
    "df9['t3_MAFLD'] = df8['t3_MAFLD']\n",
    "\n",
    "# drop these cols as those been derived for numeric cols, remain alias *_num,\n",
    "cols_to_drop_only_MAFLD = ['t1_HBsAg_x', 't2_HBsAg_x', 't1_Anti_HCV_x', 't2_Anti_HCV_x', 't1_MAFLD', 't2_MAFLD']\n",
    "cols_to_drop_fli_related = ['t1_HBsAg_x', 't2_HBsAg_x', 't1_Anti_HCV_x', 't2_Anti_HCV_x', 't1_MAFLD', 't2_MAFLD', \\\n",
    "                            't1_Triglyceride_y', 't1_BMI', 't1_gamgt', 't1_waist_y', 't1_gamgt', 't1_w', \\\n",
    "                            't2_Triglyceride_y', 't2_BMI', 't2_gamgt', 't2_waist_y', 't2_gamgt', 't2_w']\n",
    "df9_a = df9.drop(cols_to_drop_only_MAFLD, axis=1)\n",
    "\n",
    "#FLI related cols: Triglyceride_y, BMI, gamgt, waist_y, gamgt\n",
    "df9_b = df9.drop(cols_to_drop_fli_related, axis=1)\n",
    "\n",
    "df9_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling - baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_conbined shape:  (9658, 114)\n",
      "y shape:  (9658,)\n"
     ]
    }
   ],
   "source": [
    "# start modeling preparation\n",
    "# split categorical and numerical variables,\n",
    "\n",
    "df9_a.drop('t2_sex', axis=1, inplace=True)\n",
    "features = df9_a.columns.drop(['t3_MAFLD'])\n",
    "\n",
    "categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD', \\\n",
    "                        't2_w', 't2_smoke', 't2_smoke_q', 't2_coffee', 't2_betel', 't2_DM_determine', 't2_CKD']\n",
    "numeric_features = df9_a.columns.drop(categorical_features).drop(['t3_MAFLD'])\n",
    "X_categorical = df9_a[categorical_features]\n",
    "X_numeric = df9_a[numeric_features]\n",
    "y = df9_a['t3_MAFLD']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)\n",
    "\n",
    "print('X_conbined shape: ',X_combined.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic model(all important factors) AUC score:  0.8495972230855735\n",
      "Baseline Random Forest(all important factors) AUC score:  0.8443908617250324\n"
     ]
    }
   ],
   "source": [
    "# train test split, seed=2023\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=2023)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Baseline Logistic model(all important factors) AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Baseline Random Forest(all important factors) AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1_sex</th>\n",
       "      <th>t1_age</th>\n",
       "      <th>t1_waist_y</th>\n",
       "      <th>t1_Glucose_AC_y</th>\n",
       "      <th>t1_Triglyceride_y</th>\n",
       "      <th>t1_HDL_C_y</th>\n",
       "      <th>t1_AST_GOT</th>\n",
       "      <th>t1_ALT_GPT</th>\n",
       "      <th>t1_gamgt</th>\n",
       "      <th>t1_Insulin</th>\n",
       "      <th>...</th>\n",
       "      <th>t1_sarcf</th>\n",
       "      <th>t1_ms2</th>\n",
       "      <th>t1_MNA</th>\n",
       "      <th>t1_AUDIT</th>\n",
       "      <th>t1_HBV_</th>\n",
       "      <th>t1_HCV_</th>\n",
       "      <th>t1_CKD</th>\n",
       "      <th>t1_HBsAg_x_num</th>\n",
       "      <th>t1_Anti_HCV_x_num</th>\n",
       "      <th>t3_MAFLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>82.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>56.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   t1_sex  t1_age  t1_waist_y  t1_Glucose_AC_y  t1_Triglyceride_y  t1_HDL_C_y   \n",
       "0       0      59        82.0            101.0              264.0        56.4  \\\n",
       "\n",
       "   t1_AST_GOT  t1_ALT_GPT  t1_gamgt  t1_Insulin  ...  t1_sarcf  t1_ms2   \n",
       "0        21.0        19.0      15.0        9.18  ...       NaN       1  \\\n",
       "\n",
       "   t1_MNA  t1_AUDIT  t1_HBV_  t1_HCV_  t1_CKD  t1_HBsAg_x_num   \n",
       "0     NaN       NaN      0.0      0.0       2            0.44  \\\n",
       "\n",
       "   t1_Anti_HCV_x_num  t3_MAFLD  \n",
       "0               0.04         0  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis 1: The impact of interval years\n",
    "# Start with df9_a, dataset contains BMI, etc variables\n",
    "# Check t1->t3\n",
    "def select_columns(df, prefix, additional_column=None):\n",
    "    selected_columns = [col for col in df.columns if col.startswith(prefix)]\n",
    "    if additional_column is not None:\n",
    "        selected_columns.append(additional_column)\n",
    "    return df[selected_columns]\n",
    "\n",
    "df10_a_t1 = select_columns(df9_a, 't1_', 't3_MAFLD')\n",
    "df10_a_t1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_conbined shape:  (9658, 58)\n",
      "y shape:  (9658,)\n"
     ]
    }
   ],
   "source": [
    "# start modeling preparation\n",
    "# split categorical and numerical variables,\n",
    "features = df10_a_t1.columns.drop(['t3_MAFLD'])\n",
    "\n",
    "categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD']\n",
    "                        # 't2_w', 't2_smoke', 't2_smoke_q', 't2_coffee', 't2_betel', 't2_DM_determine', 't2_CKD']\n",
    "numeric_features = df10_a_t1.columns.drop(categorical_features).drop(['t3_MAFLD'])\n",
    "X_categorical = df10_a_t1[categorical_features]\n",
    "X_numeric = df10_a_t1[numeric_features]\n",
    "y = df10_a_t1['t3_MAFLD']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)\n",
    "\n",
    "print('X_conbined shape: ',X_combined.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train test split with Random Seed = 2023\n",
      "Logistic model(all important factors) for t1 predict t3 AUC score:  0.8412372357438729\n",
      "Random Forest(all important factors) for t1 predict t3 AUC score:  0.8345329991692992\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=2023)\n",
    "print(\"Start train test split with Random Seed = 2023\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Logistic model(all important factors) for t1 predict t3 AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Random Forest(all important factors) for t1 predict t3 AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train test split with Random Seed = 713\n",
      "Logistic model(all important factors) for t1 predict t3 AUC score:  0.8411811108607987\n",
      "Random Forest(all important factors) for t1 predict t3 AUC score:  0.8286175196324223\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=713)\n",
    "print(\"Start train test split with Random Seed = 713\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Logistic model(all important factors) for t1 predict t3 AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Random Forest(all important factors) for t1 predict t3 AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t2_age</th>\n",
       "      <th>t2_waist_y</th>\n",
       "      <th>t2_Glucose_AC_y</th>\n",
       "      <th>t2_Triglyceride_y</th>\n",
       "      <th>t2_HDL_C_y</th>\n",
       "      <th>t2_AST_GOT</th>\n",
       "      <th>t2_ALT_GPT</th>\n",
       "      <th>t2_gamgt</th>\n",
       "      <th>t2_Insulin</th>\n",
       "      <th>t2_T_Cholesterol</th>\n",
       "      <th>...</th>\n",
       "      <th>t2_sarcf</th>\n",
       "      <th>t2_ms2</th>\n",
       "      <th>t2_MNA</th>\n",
       "      <th>t2_AUDIT</th>\n",
       "      <th>t2_HBV_</th>\n",
       "      <th>t2_HCV_</th>\n",
       "      <th>t2_CKD</th>\n",
       "      <th>t2_HBsAg_x_num</th>\n",
       "      <th>t2_Anti_HCV_x_num</th>\n",
       "      <th>t3_MAFLD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>83.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>55.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>312.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   t2_age  t2_waist_y  t2_Glucose_AC_y  t2_Triglyceride_y  t2_HDL_C_y   \n",
       "0      60        83.0            104.0              255.0        55.8  \\\n",
       "\n",
       "   t2_AST_GOT  t2_ALT_GPT  t2_gamgt  t2_Insulin  t2_T_Cholesterol  ...   \n",
       "0        15.0        18.0      14.0        5.86             312.0  ...  \\\n",
       "\n",
       "   t2_sarcf  t2_ms2  t2_MNA  t2_AUDIT  t2_HBV_  t2_HCV_  t2_CKD   \n",
       "0       NaN       1     NaN       1.0      NaN      NaN       1  \\\n",
       "\n",
       "   t2_HBsAg_x_num  t2_Anti_HCV_x_num  t3_MAFLD  \n",
       "0             NaN                NaN         0  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check t2 ->t3\n",
    "df10_a_t2 = select_columns(df9_a, 't2_', 't3_MAFLD')\n",
    "df10_a_t2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_conbined shape:  (9658, 56)\n",
      "y shape:  (9658,)\n"
     ]
    }
   ],
   "source": [
    "# start modeling preparation for t2\n",
    "# split categorical and numerical variables,\n",
    "features = df10_a_t2.columns.drop(['t3_MAFLD'])\n",
    "\n",
    "categorical_features = ['t2_w', 't2_smoke', 't2_smoke_q', 't2_coffee', 't2_betel', 't2_DM_determine', 't2_CKD']\n",
    "numeric_features = df10_a_t2.columns.drop(categorical_features).drop(['t3_MAFLD'])\n",
    "X_categorical = df10_a_t2[categorical_features]\n",
    "X_numeric = df10_a_t2[numeric_features]\n",
    "y = df10_a_t2['t3_MAFLD']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)\n",
    "\n",
    "print('X_conbined shape: ',X_combined.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train test split with Random Seed = 2023\n",
      "Logistic model(all important factors) for t2 predict t3 AUC score:  0.8437356958391491\n",
      "Random Forest(all important factors) for t2 predict t3 AUC score:  0.8342264309811876\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=2023)\n",
    "print(\"Start train test split with Random Seed = 2023\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "\n",
    "print(\"Logistic model(all important factors) for t2 predict t3 AUC score: \",auc_log)\n",
    "print(\"Random Forest(all important factors) for t2 predict t3 AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train test split with Random Seed = 713\n",
      "Logistic model(all important factors for t2 predict t3 AUC score:  0.8475780823862711\n",
      "Random Forest(all important factors) for t1 predict t3 AUC score:  0.8398109521425979\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=713)\n",
    "print(\"Start train test split with Random Seed = 713\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Logistic model(all important factors for t2 predict t3 AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Random Forest(all important factors) for t1 predict t3 AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-135-ec82a421cf87>:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_addSurvey_1['t3_MAFLD'] = df8['t3_MAFLD']\n"
     ]
    }
   ],
   "source": [
    "# Analysis 2: the impact of survey data\n",
    "# Feature selection - added survey data then modeling\n",
    "\n",
    "columns = [\"sex\", \"age\", \"waist_y\", \"Glucose_AC_y\", \"Triglyceride_y\", \"HDL_C_y\", \"AST_GOT\", \"ALT_GPT\", \\\n",
    "          \"gamgt\", \"Insulin\", \"T_Cholesterol\", \"LDL_C_direct\", \"VLDL_C\", \"Non_HDL_C\", \"T_CHOL_HDL_C\", \\\n",
    "          \"LDL_C_HDL_C\", \"HS_CRP\", \"Hb_A1c\", \"Uric_Acid\", \\\n",
    "        #   \"HBsAg_x\", \"Anti_HCV_x\", \\\n",
    "          \"HOMA_IR\", \"Adiponectin\", \\\n",
    "           \"Leptin\", \"TotalVitaminD\", \\\n",
    "          #  \"smoke\", \"smoke_q\", \"coffee\", \"betel\",\n",
    "           \"BMI\", \"DM_determine\", \"w\", \"hyper\", \\\n",
    "           \"fg\", \"HDL\", \"trig\", \"sarcf\", \"ms2\", \"MNA\", \"AUDIT\", \"HBV_\", \"HCV_\", \"MAFLD\", \"CKD\", \\\n",
    "           'HBsAg_x_num', 'Anti_HCV_x_num']\n",
    "\n",
    "survey_data =['smoke', 's_smoke', 'smoke_t', 'smoke_q', 'smoke_qt1', 'smoke_qt2', 'smoke_f', 'smoke_n', 'smoke_second', \\\n",
    "           'tea', 'tea_c', 'tea_1', 'tea_t', 'tea_q', 'tea_f', 'tea_v', 'coffee', 'coffee_c', 'coffee_t', 'coffee_q', 'coffee_f', 'coffee_v', \\\n",
    "           'betel', 's_betel', 'betel_clt1', 'betel_clt2', 'betel_clt3', 'betel_clt4', 'betel_clt5', 'betel_clt6', 'betel_o', 'betel_t', 'betel_q', 'betel_f', 'betel_n', \\\n",
    "           'activity_t', 'carryh', 'sport', 'sport_1', 'sport_d', 'sport_t', 'cardio', 'hypertension', 'Dysrhythmia', 'ap', 'ami', 'Hyperlipidemia', 'HF', 'f_cardio', \\\n",
    "           'f_hyper', 'f_dys', 'f_ap', 'f_ami', 'f_lipid', 'f_hf', 'm_cardio', 'm_hyper', 'm_dys', 'm_ap', 'm_ami', 'm_lipid', 'm_hf', 'bs_cardio', 'bs_hyper', 'bs_hyper_1', \\\n",
    "           'bs_dys', 'bs_dys_1', 'bs_ap', 'bs_ap1', 'bs_ami', 'bs_ami_1', 'bs_lipid', 'bs_lipid_1', 'bs_hf', 'bs_hf_1', \\\n",
    "           'chi_cardio', 'chi_hyper', 'chi_dys', 'chi_ap', 'chi_ami', 'chi_lipid', 'chi_hf', \\\n",
    "           'endocrine', 'diabetes', 'Thyroid', 'f_dm', 'f_Thyroid', 'm_dm', 'm_Thyroid', 'bs_dm', 'bs_dm_1', 'bs_Thyroid', 'bs_Thyroid_1', \\\n",
    "           'chi_dm', 'chi_Thyroid', 'pepticulcer', 'gastritis', 'hbv', 'hcv', 'hepatitis_o', 'FLD', 'fibrosis', 'Cirrhosis', 'Polyposis', 'ibs', \\\n",
    "           'f_pud', 'f_hbv', 'f_hcv', 'f_liver', 'f_fld', 'f_f', 'f_lc', 'f_polyp', 'f_ibs', 'm_pud', 'm_hbv', 'm_hcv', 'm_liver', 'm_fld', 'm_f', 'm_lc', 'm_polyp', 'm_ibs', \\\n",
    "           'bs_pud', 'bs_pud_1', 'bs_hbv', 'bs_hbv_1', 'bs_hcv', 'bs_hcv_1', 'bs_liver', 'bs_fld', 'bs_fld_1', 'bs_f', 'bs_f_1', 'bs_lc', 'bs_lc_1', \\\n",
    "           'bs_polyp', 'bs_polyp_1', 'bs_ibs', 'bs_ibs_1', 'chi_pud', 'chi_hbv', 'chi_hcv', 'chi_liver', 'chi_fld', 'chi_f', 'chi_lc', 'chi_polyp', 'chi_ibs', \\\n",
    "           'respiratory', 'tb', 'Asthma', 'apnea', 'copd', 'f_resp', 'f_Asthma', 'f_apnea', 'f_copd', 'm_resp', 'm_Asthma', 'm_apnea', 'm_copd', \\\n",
    "           'bs_resp', 'bs_Asthma', 'bs_Asthma_1', 'bs_apnea', 'bs_apnea_1', 'bs_copd', 'bs_copd_1', 'chi_resp', 'chi_Asthma', 'chi_apnea', 'chi_copd', \\\n",
    "           'anemia', 'Hemophilia', 'f_anemia', 'f_h', 'm_anemia', 'm_h', 'bs_anemia', 'bs_anemia_1', 'bs_h', 'bs_h_1', 'chi_anemia', 'chi_h', \\\n",
    "           'stroke', 'brainbleed', 'head_injury', 'epilepsy', 'Parkinsons', 'dementia', 'Huntingtons', 'f_stroke', 'f_hi', 'f_brainbleed', 'f_epilep', 'f_pd', 'f_demen', 'f_hd', \\\n",
    "           'm_stroke', 'm_hi', 'm_brainbleed', 'm_epilep', 'm_pd', 'm_demen', 'm_hd', 'bs_stroke', 'bs_stroke_1', 'bs_hi', 'bs_hi_1', 'bs_brainbleed', 'bs_brainbleed_1', 'bs_epilep', \\\n",
    "           'bs_epilep_1', 'bs_pd', 'bs_pd_1', 'bs_demen', 'bs_demen_1', 'bs_hd', 'bs_hd_1', 'chi_stroke', 'chi_hi', 'chi_brainbleed', 'chi_epilep', 'chi_pd', 'chi_demen', 'chi_hd', \\\n",
    "           'insomnia', 'Depression', 'mdd_dx', 'mental', 'f_mdd', 'f_mdd_dx', 'f_mental', 'm_mdd', 'm_mdd_dx', 'm_mental', 'bs_mdd', 'bs_mdd_1', 'bs_mdd_dx', 'bs_mental', 'chi_mdd', \\\n",
    "           'chi_mdd_dx', 'chi_mental', 're_1', 'ckd', 'dialysis', 'gout', 'stone', 'bpn', 'f_re', 'f_ckd', 'f_dialysis', 'f_stone', 'f_bpn', 'm_re', 'm_ckd', 'm_dialysis', 'm_stone', \\\n",
    "           'm_bpn', 'bs_re', 'bs_ckd', 'bs_ckd_1', 'bs_dialysis', 'bs_dialysis_1', 'bs_stone', 'bs_stone_1', 'bs_bpn', 'bs_bpn_1', 'chi_re', 'chi_ckd', 'chi_dialysis', 'chi_stone', 'chi_bpn', \\\n",
    "           'cataract', 'glau', 'retina', 'sicca', 'f_cata', 'f_glau', 'f_retina', 'f_sicca', 'm_cata', 'm_glau', 'm_retina', 'm_sicca', 'bs_cata', 'bs_cata_1', 'bs_glau', 'bs_glau_1', 'bs_retina', \\\n",
    "           'bs_retina_1', 'bs_sicca', 'bs_sicca_1', 'chi_cata', 'chi_glau', 'chi_retina', 'chi_sicca', 'cancer', 'cancer_1', 'cantime', 'cancer_mi', 'lungcan', 'breastcna', 'colican', 'gastrican', \\\n",
    "           'livercan', 'cancer_o', 'f_can', 'f_can_1', 'f_canmi', 'f_lung', 'f_breast', 'f_coli', 'f_gastric', 'f_liver_c', 'f_can_o', 'm_can', 'm_can_1', 'm_canmi', 'm_lung', 'm_breast', 'm_coli', \\\n",
    "           'm_gastric', 'm_liver_c', 'm_can_o', 'bs_can', 'bs_can_1', 'bs_canmi', 'bs_lung', 'bs_lung_1', 'bs_breast', 'bs_breast_1', 'bs_coli', 'bs_coli_1', 'bs_gastric', 'bs_gastric_1', 'bs_liver_c', \\\n",
    "           'bs_liver_c_1', 'bs_can_o', 'chi_can', 'chi_can_1', 'chi_canmi', 'chi_lung', 'chi_breast', 'chi_coli', 'chi_gastric', 'chi_liver_c', 'chi_can_o', 'ndisease', 'fn', 'mn', 'bsn', 'chin', 'UNKNOWN', \\\n",
    "           'funknown', 'munknown', 'bsunknown', 'chiunknown', 'autoimmu', 'dis_o', 'f_dis_o', 'f_disable', 'm_dis_o', 'm_disable', 'bs_dis_o', 'bs_disable', 'chi_dis_o', 'chi_disable', 'Syr_drug', 'Hypnotic', \\\n",
    "           'drug', 'drug_w', 'drug_yr', 'drug_diag', 'drug_cont', 'hyper_drug', 'hyper_drugn', 'hyper_druga', 'hyper_confir', 'hyper_cont', 'dm_drug', 'dm_w', 'dm_yr', 'dm_diag', 'dm_cont', 'hor_drug', 'hor_w', \\\n",
    "           'hor_yr', 'hor_diag', 'hor_diag1', 'hor_cont', 'lipiddrug', 'lipidw', 'lipidyr', 'lipiddiag', 'lipiddiag1', 'lipidcont', 'supply', 'gasdrug', 'cmed', 'hf_none', 'hf_vit', 'hf_wgl', 'hf_foil', 'hf_ca', \\\n",
    "           'hf_p', 'hf_p1', 'hf_p2', 'hf_chic', 'hf_clam', 'hf_e', 'hf_lutein', 'hf_collagen', 'hf_other', 'hf_fmed', 'hf_unknown', 'hf_1', \\\n",
    "           'plastic_1', 'plastic_2', 'plastic_3', 'plastic_4', 'plastic_5', 'plastic_6', 'plastic_7', 'BI_1', 'BI_2', 'BI_3', \\\n",
    "           'SF36_1', 'SF36_2', 'sf36_3_a', 'sf36_3_b', 'sf36_3_c', 'sf36_3_d', 'sf36_3_e', 'sf36_3_f', 'sf36_3_g', 'sf36_3_h', 'sf36_3_i',\\\n",
    "           'sf36_3_j', 'sf36_4_a', 'sf36_4_b', 'sf36_4_c', 'sf36_4_d', 'sf36_5_a', 'sf36_5_b', 'sf36_5_c', 'sf36_6', 'sf36_7', 'sf36_8', 'sf36_9_a', 'sf36_9_b', \\\n",
    "           'sf36_9_c', 'sf36_9_d', 'sf36_9_e', 'sf36_9_f', 'sf36_9_g', 'sf36_9_h', 'sf36_9_i', 'sf36_10', 'sf36_11_a', 'sf36_11_b', 'sf36_11_c', 'sf36_11_d', \\\n",
    "           'check5', 'beda', 'bedb', 'bedt', 'waketa', 'waketb', 'sleepta', 'sleeptb', 'sleep1', 'sleep2', 'sleep3', 'sleep4', 'sleep5', 'sleep6', 'sleep7', 'sleep8', 'sleep9', 'sleep10', 'sleep10a', \\\n",
    "           'sleep11', 'sleep11v3', 'sleep12', 'sleep12v3', 'sleep13', 'sleep13v3', 'sleep14', 'sleep14v3', 'sleep15', 'check6', 'hads_1', 'hads_2', 'hads_3', 'hads_4', 'hads_5', 'hads_6', 'hads_7', 'hads_8', \\\n",
    "           'hads_9', 'hads_10', 'hads_11', 'hads_12', 'hads_13', 'hads_14', 'check7', 'uls8_1', 'uls8_2', 'uls8_3', 'uls8_4', 'uls8_5', 'uls8_6', 'uls8_7', 'uls8_8', 'check8', 'ad8_1', 'ad8_2', 'ad8_3', 'ad8_4', \\\n",
    "           'ad8_5', 'ad8_6', 'ad8_7', 'ad8_8', 'check9', 'ecog12a', 'ecog12a1', 'ecog12b1', 'ecog12b2', 'ecog12b3', 'ecog12b4', 'ecog12b5', 'ecog12b6', 'ecog12b7', 'ecog12b8', 'ecog12b9', 'ecog12b10', 'ecog12b11', 'ecog12b12', \\\n",
    "           'check10', 'mna_1', 'mna_2', 'mna_3', 'mna_4', 'mna_5', 'mna_6', 'mna_7', 'mna_8', 'mna_9', 'mna_10', 'mna_11', 'mna_12', 'mna_13', 'mna_14', 'mna_15', 'check11', 'sarcf_1', 'sarcf_2', 'sarcf_3', 'sarcf_4', 'sarcf_5', \\\n",
    "           'check12', 'd2_drink', 'd2_drinkt', 'd2_drinkav', 'd2_drinkday', 'd2_drinkcat', 'd2_drinknote', 'd2_drinkvol', 'past2_drinkav', 'past2_drinkday', 'past2_drink', 'past2_drinkcat', 'past2_drinknote', 'past2_drinkvol', 'past1_drink', \\\n",
    "           'd2_drinks', 'drink_k1', 'drink_k2', 'drink_k3', 'drink_k4', 'drink_k5', 'drink_k6', 'drink_k7', 'drink_k7_o', 'drink_k7_1', 'drink_k7_2', 'drink_k7_3', 'drink_k7_4', 'drink_k7_5', 'drink_k7_6', 'drink_t', 'drink_q', 'drink_qt1', 'drink_qt2', \\\n",
    "           'drink_red', 'B_1_1', 'B_1_2', 'B_1_3', 'B_1_4', 'B_1_5', 'B_1_6', 'B_1_7', 'B_1_8', 'B_1_9', 'B_1_10', 'B_2_1', 'B_2_2', 'B_2_3', 'B_2_4', 'check13', \\\n",
    "           'life1', 'life2', 'life3', 'life4', 'life5', 'life6', 'life7', 'life8', 'life9', 'life10', 'life11', 'life12', 'life13', 'life14', 'life15', 'life16', 'life17', 'life18', 'life19', 'life20', 'life21', 'life22', 'life23', 'life24', \\\n",
    "           'depres_11', 'depres_12', 'depres_13', 'depres_14', 'depres_15', 'depres_16', 'depres_17', 'depres_18', 'depres_19', 'depres_110', 'depres_111', 'depres_112', 'depres_113', 'depres_114', 'depres_115', 'depres_116', 'depres_117', 'depres_118', \\\n",
    "           'health_1_old', 'health_2_old', 'health_3_old', 'health_4_old', 'health_5_old', 'health_6_old', 'health_7_old', 'health_8_old', 'health_9_old', 'health_10_old', \\\n",
    "           'health_1', 'health_2', 'health_3', 'health_4', 'health_5', 'health_6', 'health_7', 'health_8', 'health_9', 'health_10'\n",
    "           ]\n",
    "\n",
    "selected_columns = columns + survey_data\n",
    "prefixes = [\"t1_\", \"t2_\"]\n",
    "renamed_columns = utils.add_prefix(selected_columns, prefixes)\n",
    "\n",
    "df_addSurvey_1 = df8[renamed_columns]\n",
    "df_addSurvey_1['t3_MAFLD'] = df8['t3_MAFLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns to remove with high missing values:  Index(['t1_sarcf', 't1_MNA', 't1_smoke_qt1', 't1_smoke_qt2', 't1_smoke_f',\n",
      "       't1_tea_1', 't1_betel_o', 't1_betel_q', 't1_sport_1', 't1_cardio',\n",
      "       ...\n",
      "       't2_health_1_old', 't2_health_2_old', 't2_health_3_old',\n",
      "       't2_health_4_old', 't2_health_5_old', 't2_health_6_old',\n",
      "       't2_health_7_old', 't2_health_8_old', 't2_health_9_old',\n",
      "       't2_health_10_old'],\n",
      "      dtype='object', length=830)\n"
     ]
    }
   ],
   "source": [
    "# Remove high missing value columns\n",
    "missing_threshold = 0.8\n",
    "def remove_columns_with_high_missing_values(df, threshold):\n",
    "    \"\"\"\n",
    "    Remove columns from a DataFrame that have missing values exceeding the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - threshold (float): The threshold for missing values. Columns with missing values exceeding this threshold will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - cleaned_df (DataFrame): The cleaned DataFrame with columns removed.\n",
    "    \"\"\"\n",
    "    total_missing = df.isnull().sum()  # 计算每列的缺失值数量\n",
    "    total_rows = df.shape[0]  # 数据集的总行数\n",
    "    columns_to_remove = total_missing[total_missing / total_rows > threshold].index  # 找到超过阈值的列名\n",
    "    cleaned_df = df.drop(columns=columns_to_remove)  # 删除指定列\n",
    "    print(\"columns to remove with high missing values: \", columns_to_remove)\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "df_addSurvey_2 = remove_columns_with_high_missing_values(df_addSurvey_1,missing_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in df_addSurvey_2: 599\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(df_addSurvey_2.columns)\n",
    "print(\"Number of columns in df_addSurvey_2:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_with_high_unique_values(df, threshold):\n",
    "    \"\"\"\n",
    "    Remove columns from a DataFramse that have unique values exceeding the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - threshold (int): The threshold for unique values. Columns with unique values exceeding this threshold will be removed.\n",
    "\n",
    "    Returns:\n",
    "    - cleaned_df (DataFrame): The cleaned DataFrame with columns removed.\n",
    "    \"\"\"\n",
    "    categorical_columns = df.select_dtypes(include='object').columns  # 获取分类字段列\n",
    "    columns_to_remove = []\n",
    "    for column in categorical_columns:\n",
    "        unique_values = df[column].nunique()  # 计算唯一值数量\n",
    "        if unique_values > threshold:\n",
    "            columns_to_remove.append(column)\n",
    "    cleaned_df = df.drop(columns=columns_to_remove)  # 删除指定列\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "threshold = 6\n",
    "df_addSurvey_3 = remove_columns_with_high_unique_values(df_addSurvey_2, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in df_addSurvey_3: 552\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(df_addSurvey_3.columns)\n",
    "print(\"Number of columns in df_addSurvey_3:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in df_addSurvey_4: 549\n"
     ]
    }
   ],
   "source": [
    "# Extra redundant cols needs to be dropped\n",
    "# drop these cols as those been derived for numeric cols, remain alias *_num,\n",
    "cols_to_drop_only_MAFLD = ['t2_sex', 't1_MAFLD', 't2_MAFLD']\n",
    "cols_to_drop_fli_related = ['t2_sex', 't1_MAFLD', 't2_MAFLD', \\\n",
    "                            't1_Triglyceride_y', 't1_BMI', 't1_gamgt', 't1_waist_y', 't1_gamgt', 't1_w', \\\n",
    "                            't2_Triglyceride_y', 't2_BMI', 't2_gamgt', 't2_waist_y', 't2_gamgt', 't2_w']\n",
    "df_addSurvey_4 = df_addSurvey_3.drop(cols_to_drop_only_MAFLD, axis=1)\n",
    "\n",
    "# #FLI related cols: Triglyceride_y, BMI, gamgt, waist_y, gamgt\n",
    "# df_addSurvey_4b = df_addSurvey_3.drop(cols_to_drop_fli_related, axis=1)\n",
    "\n",
    "\n",
    "num_columns = len(df_addSurvey_4.columns)\n",
    "print(\"Number of columns in df_addSurvey_4:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"t1_\", \"t2_\"]\n",
    "all_survey_data_renamed_cols = utils.add_prefix(survey_data, prefixes)\n",
    "\n",
    "#get final_survey_data_included_cols for modeling to define categorical variables\n",
    "final_survey_data_included_cols = [col for col in df_addSurvey_4.columns if col in all_survey_data_renamed_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks \n",
    "'t1_smoke' in final_survey_data_included_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_conbined shape:  (9658, 4825)\n",
      "y shape:  (9658,)\n"
     ]
    }
   ],
   "source": [
    "#Modeling\n",
    "# start modeling preparation\n",
    "# split categorical and numerical variables,\n",
    "features = df_addSurvey_4.columns.drop(['t3_MAFLD'])\n",
    "\n",
    "# categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD', \\\n",
    "#                         't2_w', 't2_smoke', 't2_smoke_q', 't2_coffee', 't2_betel', 't2_DM_determine', 't2_CKD']\n",
    "\n",
    "def generate_column_names(prefix, column_names):\n",
    "    new_column_names = []\n",
    "    for p in prefix:\n",
    "        new_column_names.extend([p + col for col in column_names])\n",
    "    return new_column_names\n",
    "\n",
    "prefix = ['t1_', 't2_']\n",
    "# categorical_column_names = ['sex', 'w', 'smoke', 'smoke_q', 'coffee', 'betel', 'DM_determine', 'CKD']\n",
    "categorical_column_names = ['w', 'DM_determine', 'CKD']\n",
    "categorical_column_names_non_survey = generate_column_names(prefix, categorical_column_names)\n",
    "categorical_features = categorical_column_names_non_survey + final_survey_data_included_cols + ['t1_sex']\n",
    "# selected_categorical_column_names = categorical_column_names + final_survey_data_included_cols\n",
    "# categorical_features = generate_column_names(prefix, selected_categorical_column_names)\n",
    "\n",
    "numeric_features = df_addSurvey_4.columns.drop(categorical_features).drop(['t3_MAFLD'])\n",
    "X_categorical = df_addSurvey_4[categorical_features]\n",
    "X_numeric = df_addSurvey_4[numeric_features]\n",
    "y = df_addSurvey_4['t3_MAFLD']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)\n",
    "\n",
    "print('X_conbined shape: ',X_combined.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic model(all important features) with Survey data AUC score:  0.7953911640549505\n",
      "Random Forest(all important factors) with Survey data AUC score:  0.8395641787080623\n"
     ]
    }
   ],
   "source": [
    "# train test split, seed=2023\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=2023)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Logistic model(all important features) with Survey data AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Random Forest(all important factors) with Survey data AUC score: \", auc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-148-58cdf7098124>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_addFeatures_1['t3_MAFLD'] = df8['t3_MAFLD']\n"
     ]
    }
   ],
   "source": [
    "#check unique values\n",
    "# Analysis 3: Impact of additional features that included in other papers but not in our model\n",
    "#  adding features: bloodtype, bloodtype_rh, Eosinophil, height_y, weight_y, MCV, Leukocyte, SP_Gravity, systolic_y, diastolic_y, Total_Bilirubin, Total_Protein, Glucose,Platelets, Albumin, Bilirubin, hypertension,  Hematocrit, WBCb\n",
    "# Select key columns for conventional machine learning models\n",
    "\n",
    "columns = [\"sex\", \"age\", \"waist_y\", \"Glucose_AC_y\", \"Triglyceride_y\", \"HDL_C_y\", \"AST_GOT\", \"ALT_GPT\", \\\n",
    "          \"gamgt\", \"Insulin\", \"T_Cholesterol\", \"LDL_C_direct\", \"VLDL_C\", \"Non_HDL_C\", \"T_CHOL_HDL_C\", \\\n",
    "          \"LDL_C_HDL_C\", \"HS_CRP\", \"Hb_A1c\", \"Uric_Acid\", \"HBsAg_x_num\", \"Anti_HCV_x_num\", \"HOMA_IR\", \"Adiponectin\", \\\n",
    "           \"Leptin\", \"TotalVitaminD\", \"smoke\", \"smoke_q\", \"coffee\", \"betel\", \"BMI\", \"DM_determine\", \"w\", \"hyper\", \\\n",
    "           \"fg\", \"HDL\", \"trig\", \"sarcf\", \"ms2\", \"MNA\", \"AUDIT\", \"HBV_\", \"HCV_\", \"MAFLD\", \"CKD\"]\n",
    "\n",
    "adding_features = ['bloodtype', 'bloodtype_rh', 'Eosinophil', 'height_y', 'weight_y', 'MCV', 'Leukocyte', 'SP_Gravity', \\\n",
    "                   'systolic_y', 'diastolic_y', 'Total_Bilirubin', 'Total_Protein', 'Glucose', 'Platelets', 'Albumin', \\\n",
    "                   'Bilirubin', 'hypertension',  'Hematocrit', 'WBCb']\n",
    "\n",
    "selected_features = columns + adding_features\n",
    "prefixes = [\"t1_\", \"t2_\"]\n",
    "renamed_columns = utils.add_prefix(selected_features, prefixes)\n",
    "\n",
    "df_addFeatures_1 = df8[renamed_columns]\n",
    "df_addFeatures_1['t3_MAFLD'] = df8['t3_MAFLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1_Leukocyte\n",
       "Negative    7002\n",
       "Trace       1186\n",
       "1+           725\n",
       "2+           403\n",
       "3+           191\n",
       "NaN          110\n",
       "陰性            36\n",
       "NEGATIVE       3\n",
       "NONE           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks\n",
    "# addtional Categorical features\n",
    "df_addFeatures_1['t1_Leukocyte'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values:\n",
      " ['Negative' 'Trace' '1+' '2+' nan '3+' '陰性' 'NEGATIVE' 'NONE']\n"
     ]
    }
   ],
   "source": [
    "unique_values = df_addFeatures_1['t1_Leukocyte'].unique()\n",
    "print(\"Unique Values:\\n\", unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1_Bilirubin\n",
       "Negative    9256\n",
       "NEGATIVE     234\n",
       "NaN          110\n",
       "陰性            53\n",
       "1+ (0.5)       2\n",
       "NONE           2\n",
       "3+ (4.0)       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_addFeatures_1['t1_Bilirubin'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1_Glucose\n",
       "Negative      9259\n",
       "3+ (1000)       95\n",
       "Trace(100)      75\n",
       "陰性              50\n",
       "2+ (500)        33\n",
       "1+ (250)        29\n",
       "NEGATIVE         3\n",
       "NONE             2\n",
       "3+(1000)         1\n",
       "1+(250)          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_addFeatures_1['t1_Glucose'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Categorical Values\n",
    "df_addFeatures_2 = df_addFeatures_1.copy()\n",
    "\n",
    "df_addFeatures_2['t1_Leukocyte'] = df_addFeatures_2['t1_Leukocyte'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n",
    "df_addFeatures_2['t2_Leukocyte'] = df_addFeatures_2['t2_Leukocyte'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n",
    "\n",
    "df_addFeatures_2['t1_Bilirubin'] = df_addFeatures_2['t1_Bilirubin'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n",
    "df_addFeatures_2['t2_Bilirubin'] = df_addFeatures_2['t2_Bilirubin'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n",
    "\n",
    "df_addFeatures_2['t1_Glucose'] = df_addFeatures_2['t1_Bilirubin'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n",
    "df_addFeatures_2['t2_Glucose'] = df_addFeatures_2['t2_Bilirubin'].replace({'NONE': np.nan, '陰性': 'Negative', 'NEGATIVE': 'Negative'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values:\n",
      " ['Negative' 'Trace' '1+' '2+' nan '3+']\n"
     ]
    }
   ],
   "source": [
    "# Output checks\n",
    "unique_values = df_addFeatures_2['t1_Leukocyte'].unique()\n",
    "print(\"Unique Values:\\n\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added numerical value checks\n",
    "\n",
    "def analyze_column(df, column_name):\n",
    "    non_numeric_values = df[column_name][~df[column_name].apply(lambda x: isinstance(x, (int, float)))]\n",
    "    unique_non_numeric_values = non_numeric_values.unique()\n",
    "    non_numeric_ratio = len(non_numeric_values) / len(df[column_name])\n",
    "    null_ratio = df[column_name].isnull().mean()\n",
    "    \n",
    "    output = f\"Column: {column_name}\\n\"\n",
    "    output += \"Non-numeric Values:\\n\" + str(unique_non_numeric_values) + \"\\n\"\n",
    "    output += \"Non-numeric Ratio: \" + str(non_numeric_ratio) + \"\\n\"\n",
    "    output += \"Null Ratio: \" + str(null_ratio) + \"\\n\"\n",
    "    output += \"\\n\"\n",
    "    \n",
    "    with open(\"numeric_check_output.txt\", \"a\") as file:\n",
    "        file.write(output)\n",
    "\n",
    "columns_to_analyze = ['Eosinophil', 'height_y', 'weight_y', 'MCV', 'SP_Gravity', 'systolic_y', 'diastolic_y', 'Hematocrit', 'WBCb', 'Total_Bilirubin','Total_Protein', 'Platelets', 'Albumin']\n",
    "\n",
    "prefixes = [\"t1_\", \"t2_\"]\n",
    "renamed_columns_to_analyze = utils.add_prefix(columns_to_analyze, prefixes)\n",
    "\n",
    "for column in renamed_columns_to_analyze:\n",
    "    analyze_column(df_addFeatures_1, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional summary for numeric features\n",
    "non_numeric_gt_1 = [column for column in renamed_columns_to_analyze if len(df_addFeatures_1[column][~df_addFeatures_1[column].apply(lambda x: isinstance(x, (int, float)))].unique()) > 1]\n",
    "non_numeric_ratio_gt_01 = [column for column in renamed_columns_to_analyze if (len(df_addFeatures_1[column][~df_addFeatures_1[column].apply(lambda x: isinstance(x, (int, float)))]) / len(df_addFeatures_1[column])) > 0.1]\n",
    "null_ratio_gt_01 = [column for column in renamed_columns_to_analyze if df_addFeatures_1[column].isnull().mean() > 0.1]\n",
    "\n",
    "output_additional = f\"non_numerical values 的值大于1的字段有：{non_numeric_gt_1}\\n\"\n",
    "output_additional += f\"Non-numeric Ratio 大于0.1的字段有：{non_numeric_ratio_gt_01}\\n\"\n",
    "output_additional += f\"Null Ratio 大于0.1的字段有：{null_ratio_gt_01}\\n\"\n",
    "\n",
    "with open(\"numeric_check_output.txt\", \"a\") as file:\n",
    "    file.write(output_additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column: t1_Eosinophil Null Ratio: 0.4176848208738869 - needs check null value imputation\n",
    "#Column: t1_SP_Gravity Non-numeric Values: ['>=1.030' '<=1.005' 'NONE'] - needs double check how to impute these values\n",
    "# non_numerical values 的值大于1的字段有：['t1_SP_Gravity', 't1_Total_Bilirubin', 't2_SP_Gravity', 't2_Total_Bilirubin', 't2_Total_Protein']\n",
    "# Non-numeric Ratio 大于0.1的字段有：[]\n",
    "# Null Ratio 大于0.1的字段有：['t1_Eosinophil', 't2_Eosinophil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in df_addFeatures_3: 126\n"
     ]
    }
   ],
   "source": [
    "df_addFeatures_3 = df_addFeatures_2.drop('t2_sex', axis=1)\n",
    "\n",
    "num_columns = len(df_addFeatures_3.columns)\n",
    "print(\"Number of columns in df_addFeatures_3:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting values in column: t1_Leukocyte\n",
      "Error converting values in column: t1_SP_Gravity\n",
      "Error converting values in column: t1_Total_Bilirubin\n",
      "Error converting values in column: t1_Glucose\n",
      "Error converting values in column: t1_Bilirubin\n",
      "Error converting values in column: t2_Leukocyte\n",
      "Error converting values in column: t2_SP_Gravity\n",
      "Error converting values in column: t2_Total_Bilirubin\n",
      "Error converting values in column: t2_Total_Protein\n",
      "Error converting values in column: t2_Glucose\n",
      "Error converting values in column: t2_Bilirubin\n"
     ]
    }
   ],
   "source": [
    "for column in df_addFeatures_3.columns:\n",
    "    try:\n",
    "        df_addFeatures_3[column] = pd.to_numeric(df_addFeatures_3[column], errors='raise')\n",
    "    except ValueError:\n",
    "        print(f\"Error converting values in column: {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addFeatures_4 = df_addFeatures_3.copy()\n",
    "\n",
    "columns_to_convert = ['t1_SP_Gravity', 't1_Total_Bilirubin', 't2_SP_Gravity', 't2_Total_Bilirubin', 't2_Total_Protein']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_addFeatures_4[column] = pd.to_numeric(df_addFeatures_4[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of columns to check:\n",
      "t1_Leukocyte : object\n",
      "t1_SP_Gravity : float64\n",
      "t1_Total_Bilirubin : float64\n",
      "t1_Glucose : object\n",
      "t1_Bilirubin : object\n",
      "t2_Leukocyte : object\n",
      "t2_SP_Gravity : float64\n",
      "t2_Total_Bilirubin : float64\n",
      "t2_Total_Protein : float64\n",
      "t2_Glucose : object\n",
      "t2_Bilirubin : object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of columns to check:\")\n",
    "columns_to_check = ['t1_Leukocyte', 't1_SP_Gravity', 't1_Total_Bilirubin', 't1_Glucose', 't1_Bilirubin',\n",
    "                    't2_Leukocyte', 't2_SP_Gravity', 't2_Total_Bilirubin', 't2_Total_Protein', 't2_Glucose',\n",
    "                    't2_Bilirubin']\n",
    "for column in columns_to_check:\n",
    "    print(column, \":\", df_addFeatures_4[column].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_conbined shape:  (9658, 214)\n",
      "y shape:  (9658,)\n"
     ]
    }
   ],
   "source": [
    "#Modeling\n",
    "# start modeling preparation\n",
    "# split categorical and numerical variables,\n",
    "features = df_addFeatures_4.columns.drop(['t3_MAFLD'])\n",
    "\n",
    "# categorical_features = ['t1_sex', 't1_w', 't1_smoke', 't1_smoke_q', 't1_coffee', 't1_betel', 't1_DM_determine', 't1_CKD', \\\n",
    "#                         't2_w', 't2_smoke', 't2_smoke_q', 't2_coffee', 't2_betel', 't2_DM_determine', 't2_CKD']\n",
    "\n",
    "prefix = ['t1_', 't2_']\n",
    "categorical_column_names = ['w', 'smoke', 'smoke_q', 'coffee', 'betel', 'DM_determine', 'CKD', \\\n",
    "                            'bloodtype', 'bloodtype_rh', 'Leukocyte', 'Bilirubin', 'Glucose', 'hypertension']\n",
    "categorical_features = generate_column_names(prefix, categorical_column_names) + ['t1_sex']\n",
    "\n",
    "\n",
    "numeric_features = df_addFeatures_4.columns.drop(categorical_features).drop(['t3_MAFLD'])\n",
    "X_categorical = df_addFeatures_4[categorical_features]\n",
    "X_numeric = df_addFeatures_4[numeric_features]\n",
    "y = df_addFeatures_4['t3_MAFLD']\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
    "\n",
    "# Missing value handling\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_numeric_scaled_imputed = imputer.fit_transform(X_numeric_scaled)\n",
    "\n",
    "#dummy var\n",
    "X_categorical_str = X_categorical.astype(str)\n",
    "X_categorical_encoded = pd.get_dummies(X_categorical_str)\n",
    "\n",
    "# concat\n",
    "X_numeric_scaled_imputed = pd.DataFrame(X_numeric_scaled_imputed, columns=X_numeric.columns)\n",
    "X_numeric_scaled_imputed.reset_index(drop=True, inplace=True)\n",
    "X_categorical_encoded.reset_index(drop=True, inplace=True)\n",
    "X_combined = pd.concat([X_numeric_scaled_imputed, X_categorical_encoded], axis=1)\n",
    "\n",
    "print('X_conbined shape: ',X_combined.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic model(all important features) with more features added data AUC score:  0.8659901446097683\n",
      "Random Forest(all important factors) with Survey data AUC score:  0.8539513390107312\n"
     ]
    }
   ],
   "source": [
    "# train test split, seed=2023\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.3, random_state=2023)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#Log model\n",
    "logmodel = LogisticRegression(max_iter=2000)\n",
    "logmodel.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "predictions_log = logmodel.predict(X_test)\n",
    "probabilities_log = logmodel.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "\n",
    "# Calculate AUC\n",
    "auc_log = roc_auc_score(y_test, probabilities_log)\n",
    "print(\"Logistic model(all important features) with more features added data AUC score: \",auc_log)\n",
    "\n",
    "# Random forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "predictions_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = rf_model.score(X_test, y_test)\n",
    "\n",
    "# Calculate AUC\n",
    "probabilities_rf = rf_model.predict_proba(X_test)[:, 1]  # Use probabilities of the positive class\n",
    "auc_rf = roc_auc_score(y_test, probabilities_rf)\n",
    "print(\"Random Forest(all important factors) with Survey data AUC score: \", auc_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
